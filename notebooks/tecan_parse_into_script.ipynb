{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script Name   : parse_plot_tecan_data.py\n",
    "# Author        : Pepe Flores & Ricardo Martinez\n",
    "# Created       : 12th June 2019\n",
    "\n",
    "# Description   : This script will parse tecan data and output scatterplots of the parsed data\n",
    "\n",
    "\n",
    "###########\n",
    "# imports #\n",
    "###########\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import xlrd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "%config InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "##########################\n",
    "# MAIN PARSING FUNCTIONS #\n",
    "##########################\n",
    "    \n",
    "def parse_tecan_results_files(data_folder):\n",
    "    \"\"\"\n",
    "    This script parses the tecan results files. Care should be taken when feeding the\n",
    "    script 'Single' or 'Multiple' reads as feeding both simultaneously will result\n",
    "    in a misformed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    tecan_df_list = []\n",
    "    multi_measurement_blocks = []\n",
    "    \n",
    "    for path, dirs, files in os.walk(data_folder):\n",
    "        for file in [f for f in files if '.xlsx' in f]:\n",
    "            print(file)\n",
    "            tecan_results_file = os.path.join(path, file)\n",
    "            work_sheets = xlrd.open_workbook(tecan_results_file).sheet_names()\n",
    "            print(work_sheets)\n",
    "            for sheet in work_sheets[::-1]:\n",
    "                print(sheet)\n",
    "                #tecan_worksheet = pd.read_excel(tecan_results_file, sheet_name=sheet, header=None)\n",
    "                df = pd.read_excel(tecan_results_file, sheet_name=sheet, header=None)\n",
    "                \n",
    "                #print(df)\n",
    "                \n",
    "                if df.empty:\n",
    "                    # if the excel worksheet is empty continue onto the next one\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    # if the worksheet is not empty, the script runs \n",
    "                    \n",
    "                    # measurement start time is located on located on the 1st column of the row with string \"Start Time:\"\n",
    "                    measurements_start_time = pd.to_datetime(df.loc[df[0]=='Start Time:'][1], dayfirst=True).reset_index(drop=True)\n",
    "                    \n",
    "                    'meas start time'\n",
    "                    print(measurements_start_time)\n",
    "                    exp_start_time = min(measurements_start_time)\n",
    "                    \n",
    "                    # In Tecan results file with single measurements, the results block ends with \"End Time:\"\n",
    "                    measurements_end_time = df.loc[df[0]=='End Time:'][1].index\n",
    "                    \n",
    "                    # Parsing is Multiple or Single measurements per well dependent\n",
    "                    measurement_blocks = []\n",
    "                    if \"Multiple Reads per Well (Border)\" in df.iloc[:,0].values:\n",
    "                        \n",
    "                        tecan_mode = 'multiple'\n",
    "                    \n",
    "                        #print(\"%r it's a multiple measurements tecan file\" % sheet) \n",
    "                        # Get the indexes where the value is \"Time [s]\" which indicates\n",
    "                        # the beginning of a new measurement block\n",
    "                        measurement_block_start_indices = df[df[0] == \"Time [s]\"].index\n",
    "\n",
    "                        # iterate trough the blocks\n",
    "                        for block_idx in measurement_block_start_indices:\n",
    "                            # well_ID\n",
    "                            well_idx = block_idx - 1\n",
    "                            \n",
    "                            # measurement rep index\n",
    "                            m1_idx = block_idx + 4\n",
    "                            m2_idx = block_idx + 5\n",
    "                            m3_idx = block_idx + 6\n",
    "                            m4_idx = block_idx + 7\n",
    "\n",
    "\n",
    "                            # Create a DF per measurements block where col_0 is Time [s], \n",
    "                            # and the other columns are the measurements\n",
    "                            block_df = pd.concat([df.iloc[block_idx].dropna(), \n",
    "                                                  df.iloc[m1_idx].dropna(),                                                     \n",
    "                                                  df.iloc[m2_idx].dropna(),\n",
    "                                                  df.iloc[m3_idx].dropna(),\n",
    "                                                  df.iloc[m4_idx].dropna()], axis=1)\n",
    "\n",
    "                            # Rename the columns and drop the row that was used for naming the columns\n",
    "                            block_df.columns = block_df.iloc[0].values\n",
    "                            block_df =  block_df.reindex(block_df.index.drop(0))\n",
    "                            block_df = block_df.rename(columns={'0;1':'rep1', '1;1':'rep2',\n",
    "                                                                '1;0':'rep3', '0;0':'rep4'})\n",
    "                            \n",
    "                            # Add the Well column to the block DF\n",
    "                            block_df[\"Well\"] = df.iloc[well_idx,0]\n",
    "                            block_df[\"Worksheet\"] = sheet\n",
    "                            \n",
    "                            # Convert the \"Time [s]\" column to float and then to timedelta seconds\n",
    "                            block_df[\"Time [s]\"] = block_df[\"Time [s]\"].astype('float').astype('timedelta64[s]')\n",
    "\n",
    "                            # Add the seconds to the starting time\n",
    "                            block_df[\"Measuring_time\"] = measurements_start_time.loc[0] + block_df[\"Time [s]\"]\n",
    "                            \n",
    "                            # Add the block df to the measurement_blocks list\n",
    "                            multi_measurement_blocks.append(block_df)\n",
    "\n",
    "                        # post-parsing processing of all the blocks into a dataframe\n",
    "                        measurements_df = pd.concat(multi_measurement_blocks)\n",
    "                        measurements_df[\"Filename\"] = tecan_results_file\n",
    "                        measurements_df[\"Measuring_type\"] = tecan_mode\n",
    "                        measurements_df[\"Row\"] = measurements_df.Well.apply(lambda x: split_well(x)[0])\n",
    "                        measurements_df[\"Column\"] = measurements_df.Well.apply(lambda x: int(split_well(x)[1]))\n",
    "                        \n",
    "                        measurements_df = measurements_df.melt(id_vars=[\"Filename\", \"Worksheet\", \"Well\", \"Row\",\"Column\", \n",
    "                                                                        \"Measuring_type\", \"Measuring_time\"],\n",
    "                                                               value_vars=['rep1', 'rep2', 'rep3', 'rep4'],\n",
    "                                                               var_name=\"Replicate\", value_name=\"Abs\")\n",
    "                        measurements_df['Abs'] = measurements_df['Abs'].astype(float)\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        tecan_mode='single'\n",
    "                                                \n",
    "                        # In Tecan results file with single measurements, the results block starts with \"Cycle Nr.\"\n",
    "                        measurements_start = df[df[0]=='Cycle Nr.'].index\n",
    "                        \n",
    "                        for i in range(len(measurements_start_time)):\n",
    "                            block_df = df.loc[measurements_start[i]:measurements_end_time[i]-4]\n",
    "                            # deliminate where the measurements stop for that block\n",
    "                            booldf = block_df.isnull()\n",
    "\n",
    "                            if block_df.iloc[1].isna().sum() > 0:  \n",
    "                                stop_column = booldf[booldf.any(axis=1)].idxmax(axis=1).iloc[1]\n",
    "                                block_df = block_df.loc[:,:stop_column]\n",
    "                                block_df = block_df[block_df.columns[:-1]]\n",
    "                            block_df.columns = range(len(block_df.iloc[1]))\n",
    "                            # adjust sampling_time\n",
    "                            block_df.iloc[1,1:] = measurements_start_time.iloc[i]+pd.to_timedelta(block_df.iloc[1,1:],unit='s')\n",
    "\n",
    "                            block_df = block_df.T\n",
    "                            block_df.columns = block_df.iloc[0]\n",
    "                            block_df = block_df.reindex(block_df.index.drop(0))\n",
    "\n",
    "                            measurement_blocks.append(block_df)\n",
    "                            \n",
    "                        # post-processing\n",
    "                        dfs = pd.concat(measurement_blocks)\n",
    "                        dfs = dfs.drop(labels=[\"Cycle Nr.\"], axis=1)\n",
    "                        dfs[\"Time [s]\"] = pd.to_datetime(dfs[\"Time [s]\"])\n",
    "                        dfs = dfs.reset_index(drop=True)\n",
    "                        \n",
    "                        \n",
    "                        not_wells = ['Time [s]', 'Temp. [°C]']\n",
    "                        wells = [x for x in dfs.columns if x not in not_wells]\n",
    "                        dfs = dfs.melt(value_vars=wells, id_vars=not_wells, value_name=\"Abs\", var_name=\"Well\")\n",
    "\n",
    "                        dfs[\"Row\"] = dfs.Well.apply(lambda x: split_well(x)[0])\n",
    "                        dfs[\"Column\"] = dfs.Well.apply(lambda x: int(split_well(x)[1]))\n",
    "                        dfs[\"Filename\"] = tecan_results_file\n",
    "                        dfs[\"Worksheet\"] = sheet\n",
    "                        dfs[\"Measuring_type\"] = tecan_mode\n",
    "                        dfs['Abs'] = dfs['Abs'].astype(float)\n",
    "                        tecan_df_list.append(dfs)\n",
    "\n",
    "    \n",
    "\n",
    "        if len(tecan_df_list)==0 and len(multi_measurement_blocks)==0:\n",
    "            print(\"There is no data to parse!\")\n",
    "            return pd.DataFrame() # empty dataframe\n",
    "        \n",
    "        if len(tecan_df_list) > 0: \n",
    "            \n",
    "            tecan_df = pd.concat(tecan_df_list)\n",
    "            \n",
    "            \n",
    "            tecan_df[\"t[m]\"] = (tecan_df[\"Time [s]\"] - tecan_df[\"Time [s]\"].min()).astype('timedelta64[m]')\n",
    "            tecan_df[\"Replicate\"] = 1\n",
    "            \n",
    "            tecan_df = tecan_df.rename(columns={\"Time [s]\":\"Measuring_time\"})\n",
    "            tecan_df = tecan_df.drop(labels=[\"Temp. [°C]\", \"t[m]\"], axis=1)\n",
    "\n",
    "            if len(multi_measurement_blocks) > 0:\n",
    "                tecan_df = pd.concat([tecan_df, measurements_df])\n",
    "        \n",
    "        else:\n",
    "            tecan_df = measurements_df\n",
    "\n",
    "            \n",
    "        # Convert time to min\n",
    "        experiment_start_time = min(tecan_df[\"Measuring_time\"])\n",
    "        tecan_df[\"t[m]\"] = (tecan_df[\"Measuring_time\"] - experiment_start_time).astype('timedelta64[m]')\n",
    "        tecan_df = tecan_df.sort_values([\"t[m]\", \"Column\", \"Well\"])\n",
    "        tecan_df = tecan_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "        return tecan_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/rdmtinez/Desktop/MScThesis/data/PBR/exp3/daily_samp_tecan/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day4.xlsx\n",
      "['Sheet2', 'Sheet1']\n",
      "Sheet1\n",
      "Sheet2\n",
      "0   2019-05-15 17:21:33\n",
      "1   2019-05-15 17:21:49\n",
      "2   2019-05-15 17:22:06\n",
      "3   2019-05-15 17:22:22\n",
      "Name: 1, dtype: datetime64[ns]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d17dc8978950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparse_tecan_results_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-e55df03e2de6>\u001b[0m in \u001b[0;36mparse_tecan_results_files\u001b[0;34m(data_folder)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasurements_start_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             \u001b[0mblock_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeasurements_start\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmeasurements_end_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                             \u001b[0;31m# deliminate where the measurements stop for that block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0mbooldf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/program_files/anaconda3/envs/fluxy/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3957\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3958\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "parse_tecan_results_files(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_metadata(metadata_folder):\n",
    "    \"\"\"\n",
    "    Metadata refers to the 96-Well Plate (8-row, 12-column) arrangement in\n",
    "    .csv format where one files contains name the sample housed in each \n",
    "    cell and the other the growth medium used. Ensure the provided metadata\n",
    "    has columns (1-12) and rows(A-H) labeled as well.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    files = os.listdir(metadata_folder)\n",
    "    assert ['growth-media_arrangement.csv', 'samples_arrangement.csv'] == files\n",
    "    \n",
    "    flag=False\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(metadata_folder, file), index_col=0)\n",
    "        \n",
    "        df = df.dropna().reset_index()\n",
    "\n",
    "        if 'samples' in file:\n",
    "            df = df.melt(value_vars=df.columns[1:],\n",
    "                         var_name=\"Column\", id_vars=\"index\",\n",
    "                         value_name=\"Sample\")\n",
    "        \n",
    "        else: \n",
    "            df = df.melt(value_vars=df.columns[1:],\n",
    "                         var_name=\"Column\", id_vars=\"index\", \n",
    "                         value_name=\"Media\")\n",
    "\n",
    "        df = df.rename(columns={\"index\":\"Row\"})\n",
    "        df[\"Well\"] = df[\"Row\"]+ df[\"Column\"]\n",
    "        df['Column'] = df['Column'].astype(int)\n",
    "\n",
    "        if not flag:\n",
    "            df0 = df.copy()\n",
    "            flag=True\n",
    "            \n",
    "    return  df.merge(df0[[\"Well\", \"Media\"]], on=\"Well\")\n",
    "\n",
    "def remove_blank_signal(df, mdf):\n",
    "    \"\"\"\n",
    "    This function does returns two dataframes:\n",
    "        1. df_mean --- this dataframe has the BLANK'S signal removed\n",
    "        from the sample's signal on a per time, media, and replicate \n",
    "        basis. At every timepoint a blank's well signal is measured\n",
    "        at 4 regions. Here the signals are subtracted by region and\n",
    "        then averaged .\n",
    "        \n",
    "        2. df_regs --- this dataframe has the blank's signal removed\n",
    "        from the sample's signal on a per time, media, and replicate\n",
    "        basis. Here the signals are subtracted by region, but are NOT\n",
    "        averaged. This is useful because it allows one to see the\n",
    "        differences in the TECAN region measurements\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # dataframe hosting the sample's name ID e.g. ICL_184BA or BLANK\n",
    "    df = df.merge(mdf[[\"Well\", \"Media\", \"Sample\"]], on=\"Well\", how=\"left\")\n",
    "\n",
    "    # temp dataframe consisting only of the BLANK's values\n",
    "    blank_df = df.loc[df[\"Sample\"] == \"BLANK\"].copy()\n",
    "    \n",
    "    # merges the dataframes such that all the well replictes are aligned to the\n",
    "    # blank's replicates, this makes subtraction on a per replicate basis possible\n",
    "    df = df.merge(blank_df[[\"t[m]\", \"Media\", 'Abs', 'Replicate']],\n",
    "                  how='left',\n",
    "                  on=[\"t[m]\", \"Media\", 'Replicate'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Subtraction (Sample_Abs -- Blank_Abs) then MEAN\n",
    "    dfs = df.copy()\n",
    "    dfs['Corr_Abs'] = dfs['Abs_x'] - dfs['Abs_y']\n",
    "    df_regs = dfs.drop(labels=['Abs_x', 'Abs_y'], axis=1)    \n",
    "    df_tmp = df_regs.groupby(['Well', 't[m]', 'Media']).agg('mean').reset_index()\n",
    "    \n",
    "    # only one row is needed from the df_reps as the aggregate-mean averages over 4 rows\n",
    "    # we first drop the unnecessary rows and columns, reset the index and mer\n",
    "    df_mean = df_regs[::4].drop(labels=['Replicate','Corr_Abs'], axis=1).reset_index(drop=True)\n",
    "    df_mean = df_mean.merge(df_tmp[['Well', 't[m]', 'Media', 'Corr_Abs']],\n",
    "                            how='left',\n",
    "                            on=['Well', 't[m]', 'Media'])\n",
    "    \n",
    "    return df_mean, df_regs\n",
    "\n",
    "\n",
    "def split_well(s):\n",
    "    \n",
    "    #split well into column row\n",
    "    # https://stackoverflow.com/questions/430079/how-to-split-strings-into-text-and-number\n",
    "    \n",
    "    head = s.rstrip('0123456789')\n",
    "    tail = s[len(head):]\n",
    "    return head, tail\n",
    "\n",
    "\n",
    "def create_strain_well_dictionary(df_mean):\n",
    "    \"\"\"\n",
    "    This helper function extracts the wells (e.g. A1) associated\n",
    "    with a given strain to facilitate plotting.\n",
    "    \"\"\"\n",
    "    \n",
    "    strains = df_mean['Sample'].unique()\n",
    "    \n",
    "    # strain at well dictionary\n",
    "    wells_dict = {}\n",
    "    for stn in strains:\n",
    "        wells_dict[stn] = [well for well in df_mean[df_mean['Sample']==stn]['Well'].unique()]\n",
    "        \n",
    "    return wells_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "######################\n",
    "# PLOTTING FUNCTIONS #\n",
    "######################\n",
    "    \n",
    "    \n",
    "def plot_all_96W_format(df, y_axis_var=\"Corr_Abs\"):\n",
    "    \"\"\"\n",
    "    This function plots the growth measurements in corresponding 96-Well Format,\n",
    "    i.e. (8rows x 12 cols)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cols = np.arange(1,13)\n",
    "    rows = list(map(chr, range(65, 73)))\n",
    "    \n",
    "    g = sns.FacetGrid(df, col=\"Column\", row=\"Row\", aspect=2, col_order=cols,\n",
    "                      row_order=rows, sharey=False)\n",
    "    g = (g.map(sns.scatterplot, \"t[m]\", y_axis_var, marker=\".\").set_titles(\"{row_name}{col_name}\"))\n",
    "    \n",
    "    return g\n",
    "\n",
    "\n",
    "def plot_all_regions_96W_format(df, y_axis_var=\"Corr_Abs\"):\n",
    "    \"\"\"\n",
    "    This function plots the growth measurements in corresponding 96-Well Format, \n",
    "    for each mesurement region the Tecan output. \n",
    "\n",
    "    \"\"\"\n",
    "    cols = np.arange(1,13)\n",
    "    rows = list(map(chr, range(65, 73)))\n",
    "    \n",
    "    g = sns.FacetGrid(df, col=\"Column\", row=\"Row\", aspect=2, col_order=cols,\n",
    "                      row_order=rows, hue='Replicate', sharey=False)\n",
    "    g = (g.map(sns.scatterplot, \"t[m]\", y_axis_var, marker=\".\").set_titles(\"{row_name}{col_name}\"))\n",
    "    \n",
    "    return g\n",
    "    \n",
    "\n",
    "def plot_all_strains_media_format(df, y_axis_var='Corr_Abs'):\n",
    "    \"\"\"\n",
    "    Plot the growth measurements of individual 'strain' or list of strains at\n",
    "    their respective growth conditions--i.e. growth media format i.e. (N-rows x 3cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    g=sns.FacetGrid(df, col='Media', row='Sample', aspect=2, col_order=['sup', 'tp', 'tsb'], sharey=False)\n",
    "    g=(g.map(sns.scatterplot, 't[m]', y_axis_var).set_titles('{row_name}-{col_name}'))\n",
    "    \n",
    "    return g\n",
    "\n",
    "\n",
    "def plot_select_strains_media_format(strains, df):\n",
    "    \"\"\"\n",
    "    Plot the growth measurements of individual 'strain' or list of strains at\n",
    "    their respective growth conditions--i.e. growth media format i.e. (N-rows x 3cols)\n",
    "    \"\"\"\n",
    "    \n",
    "    def plot(strains, df, y_axis_var='Corr_Abs'):\n",
    "        \"\"\"This does the actual plotting\"\"\"\n",
    "\n",
    "        wells = []\n",
    "        for s in strains:\n",
    "            \n",
    "            wells.append(wells_dict[s])\n",
    "            \n",
    "        wells = np.ravel(wells)\n",
    "        df0 = pd.DataFrame(columns=df.columns)\n",
    "        for well in wells:\n",
    "            df0 = df0.append(df[df['Well']==well], ignore_index = True) \n",
    "\n",
    "        g = sns.FacetGrid(df0, col='Media', row='Sample', aspect=2, col_order=[\"sup\", \"tp\", \"tsb\"], sharey=False)\n",
    "        g = (g.map(sns.scatterplot, 't[m]', y_axis_var).set_titles('{row_name} - {col_name}'))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "        return g\n",
    "    \n",
    "    \n",
    "    # below are the different types of input that can be used by\n",
    "    # a user, this is parsed and pass to plot\n",
    "    \n",
    "    # dictionary of strains and the wells they're mapped to\n",
    "    wells_dict = create_strain_well_dictionary(df)\n",
    "    if type(strains)==str:\n",
    "        strains = [strains]\n",
    "        \n",
    "        return plot(strains, df)\n",
    "    \n",
    "    else:\n",
    "        assert type(strains)== list\n",
    "\n",
    "        return plot(strains, df)\n",
    "\n",
    "\n",
    "########\n",
    "# MAIN #\n",
    "########\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Workhorse\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('-in', '--data_folder',\n",
    "                        required=True,\n",
    "                        help=\"Path to tecan excel data\")\n",
    "    \n",
    "    parser.add_argument('-mdf', '--metadata_folder',\n",
    "                        required=True,\n",
    "                        help='Path to metadata')\n",
    "    \n",
    "    parser.add_argument('-out','--output_folder',\n",
    "                        required=True,\n",
    "                        help='Path to save plots and parsed csv')\n",
    "\n",
    "    parser.add_argument('-p', '--plot',\n",
    "                        default=False,\n",
    "                        help = 'returns the plots for all your data if True')\n",
    "    \n",
    "    parser.add_argument('-sl', '--strain_list',\n",
    "                        required=False,\n",
    "                        nargs='+',\n",
    "                        help='returns plot for selected strains--do NOT separate with commas')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dfp = parse_tecan_results_files(args.data_folder)\n",
    "    mdf = parse_metadata(args.metadata_folder)\n",
    "    \n",
    "    \n",
    "    df_mean, df_regs = remove_blank_signal(dfp, mdf)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        df_mean.to_csv(os.path.join(args.output_folder, 'parsed_kinetic.csv'))\n",
    "        df_regs.to_csv(os.path.join(args.output_folder, 'parsed_kinetic_region.csv'))\n",
    "        \n",
    "               \n",
    "    except:\n",
    "        \n",
    "        os.mkdir(args.output_folder)\n",
    "        df_mean.to_csv(os.path.join(args.output_folder, 'parsed_kinetic.csv'))\n",
    "        df_regs.to_csv(os.path.join(args.output_folder, 'parsed_kinetic_region.csv'))\n",
    "\n",
    "\n",
    "    if args.plot:\n",
    "        \"\"\"\n",
    "        return all growth scatterplots in 96W (8 x 12) and media (32 x 3) formats\n",
    "        \"\"\"\n",
    "        \n",
    "        g = plot_all_96W_format(df_mean)\n",
    "        g.savefig(os.path.join(args.output_folder, 'all_strains_96W_format.png'))\n",
    "        \n",
    "        g = plot_all_strains_media_format(df_mean)\n",
    "        g.savefig(os.path.join(args.output_folder, 'all_strains_media_format.png'))\n",
    "        \n",
    "        g = plot_all_regions_96W_format(df_regs)\n",
    "        g.savefig(os.path.join(args.output_folder, 'all_strains_regions_96W_format.png'))\n",
    "\n",
    "    if args.strain_list:\n",
    "        \"\"\"\n",
    "        return growth scatterplots in media (n x 3) format\n",
    "        \"\"\"\n",
    "        print(args.strain_list)\n",
    "        g = plot_select_strains_media_format(args.strain_list, df_mean)\n",
    "        g.savefig(os.path.join(args.output_folder, 'spc_strains_rename.png'))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
